Line 17: (NLP)
Line 19: NLP
Line 27: NLP
Line 27: NLP
Line 27: interpretable
Line 27: NLP
Line 31: NLP
Line 31: NLP
Line 35: NLP.
Line 35: NLP.
Line 35: NLP
Line 35: NLP
Line 35: lemmatisation.
Line 35: tokenisation
Line 35: grammar-based
Line 35: co-occurrence.
Line 39: perceptron
Line 39: multilayer
Line 39: perceptrons
Line 39: gradient-based
Line 39: backpropagation.
Line 39: hyperparameters
Line 45: Joseph
Line 45: Weizenbaum
Line 45: ELIZA
Line 45: (Weizenbaum
Line 45: rule-based
Line 45: chatbot
Line 45: Eugene
Line 45: Goostman,
Line 45: chatbot,
Line 45: Turing
Line 45: Eugene
Line 45: (Vaswani
Line 45: et
Line 45: al.
Line 45: ChatGPT,
Line 45: NLP
Line 51: curd’and
Line 51: Rahul’.
Line 51: NLP
Line 57: development—they
Line 57: Alister
Line 57: Elaine
Line 57: Morgan
Line 61: sociolinguistics,
Line 61: psycholinguistics,
Line 61: neurolinguistics,
Line 61: Noam
Line 61: Chomsky
Line 61: Steven
Line 61: (Jurafsky
Line 61: Vaneechoutte
Line 67: Language-related
Line 67: (Tsujii
Line 67: (NLP),
Line 67: (AI),
Line 73: sub-field
Line 73: NLP
Line 79: NLP
Line 79: machine-readable
Line 79: NLP
Line 79: NLP-based
Line 91: (NLP).
Line 91: NLP
Line 91: sentence-level
Line 91: word-level
Line 91: node-to-edge
Line 91: underfund
Line 91: Indian
Line 91: non-exhaustive
Line 91: NLP.
Line 97: Part-of-Speech
Line 97: (POS)
Line 97: underfund
Line 97: Indian
Line 97: NLP,
Line 97: POS
Line 97: Penn
Line 97: Treebank
Line 97: POS
Line 99: (NER):
Line 99: real-world
Line 99: ‘Indian’
Line 99: ‘NORP’
Line 101: ‘underfunding’
Line 105: Hindi,
Line 111: NLP.
Line 115: Summarisation:
Line 117: underfund’?
Line 117: ‘India’.
Line 123: curation
Line 123: free-flowing
Line 123: machine-readable
Line 123: PDFs,
Line 123: (OCR)
Line 123: open-domain
Line 127: misspelt
Line 127: deduplication.
Line 127: Unicode
Line 131: Pre-processing.
Line 131: lowercasing,
Line 131: stop-word
Line 131: lemmatisation,
Line 131: one-size-fits-all
Line 131: preprocessing
Line 131: NLP
Line 135: preprocessed,
Line 135: NLP
Line 135: human-readable
Line 135: frequency-based
Line 135: one-hot
Line 135: bag-of-words
Line 135: NLP
Line 135: embeddings,
Line 135: human-readable
Line 139: NLU
Line 139: NLG,
Line 139: words/phrases/characters.
Line 139: NLP-based
Line 139: ‘hidden/latent
Line 139: (LMs).
Line 139: (RNN)
Line 139: (Elman
Line 139: Short-Term
Line 139: (LSTM),
Line 139: (GRU)
Line 139: (Gers
Line 139: et
Line 139: al.
Line 139: Tsujii
Line 139: Cho
Line 139: et
Line 139: al.
Line 139: (Vaswani
Line 139: et
Line 139: al.
Line 139: de
Line 139: facto
Line 139: today’s
Line 139: NLP.
Line 139: LMs
Line 143: F1-score
Line 143: (macro/micro),
Line 143: summarisation
Line 143: (BLEU)
Line 143: Recall-Oriented
Line 143: Gisting
Line 143: BERTScore
Line 143: LMs
Line 143: entropy-based
Line 143: LM
Line 177: hyperparameters
Line 177: open-source
Line 177: pre-processing
Line 191: Greek
Line 191: morphe,
Line 191: ology,
Line 191: Hindi,
Line 191: Turkish,
Line 191: Hungarian
Line 191: morphologically
Line 191: English
Line 191: Chinese
Line 191: morphologically
Line 191: morphologically
Line 195: English
Line 197: Hindi
Line 199: Tamil
Line 233: morphologically-poor
Line 233: (English)
Line 233: morphologically-rich
Line 233: (Hindi
Line 233: Tamil).
Line 233: Morphologically-rich
Line 237: morphologically-poor
Line 237: English,
Line 237: morphologically-rich
Line 237: Hindi,
Line 243: -ing
Line 243: words—these
Line 243: ‘ing’
Line 251: un,
Line 251: mis,
Line 251: intra,
Line 251: ing,
Line 251: ness,
Line 251: ly,
Line 259: Lemmatisation
Line 287: interpol
Line 299: Stemmer
Line 299: WordNetLemmatizer
Line 303: as-is
Line 303: (e.g.,
Line 303: pre
Line 303: preflight).
Line 303: (e.g.,
Line 303: ly
Line 303: English,
Line 309: stemmer
Line 309: derivational
Line 309: (rule-based)
Line 309: NLP
Line 309: algorithms—the
Line 309: Stemmers.
Line 313: non-meaningful
Line 313: ‘len’,
Line 313: English
Line 317: Lemmatisation
Line 319: Lemmatisers
Line 319: Lemmatisers
Line 319: stemmers
Line 319: well-defined
Line 319: lemmatiser
Line 319: lemmatisers
Line 319: similar-meaning
Line 319: WordNet
Line 319: English
Line 325: lemmatisation
Line 325: signal-to-noise
Line 325: vocabulary/lexicon
Line 325: NLP,
Line 325: NER,
Line 325: POS
Line 325: domain-specific
Line 325: (e.g.,
Line 325: AFINN,
Line 325: SentiWordNet,
Line 325: EmoLex,
Line 325: PropBank)
Line 325: ‘hangry’.
Line 329: Tokenisation
Line 331: NLP
Line 331: units/chunks
Line 331: tokenisation.
Line 335: tokenisation’.
Line 339: Sentence/Word/Character-Level
Line 339: sentence-level
Line 339: tokenisation
Line 339: tokenisation’.]
Line 339: English,
Line 339: whitespace
Line 339: ‘tokenisation.’].
Line 339: ‘tokenisation.’
Line 339: tokenised
Line 339: ‘tokenisation’,
Line 339: character-level
Line 343: N-grams.
Line 343: uni-gram,
Line 343: tokenisation
Line 343: n-grams
Line 343: word-level
Line 343: tokenisation’,
Line 343: ‘tokenisation
Line 343: <EOS>’],
Line 343: <EOS>
Line 343: n-gram
Line 343: n-grams
Line 343: data-specific.
Line 347: Subword
Line 347: Tokenisation
Line 349: character-level
Line 349: subword
Line 349: ‘Kendall’,
Line 349: tokenisation
Line 349: sub-word
Line 349: tokenisation,
Line 349: bottom-up
Line 349: subword
Line 349: tokenisation
Line 349: subword
Line 349: occurrence—Byte
Line 349: Wordpiece
Line 349: Tokenisation.
Line 353: (BPE)
Line 355: (Gage
Line 355: encode/compress
Line 359: 2nd
Line 363: FCBPE
Line 367: BPE.
Line 371: (pre-tokenisation):
Line 375: ‘ok‘.
Line 375: ok
Line 375: ok
Line 375: ok
Line 375: ok
Line 375: {‘ok’}
Line 379: ‘ok‘
Line 379: ‘ok’
Line 379: ‘tok
Line 379: ‘tok
Line 379: ‘tok
Line 379: ‘tok
Line 379: ‘tok’
Line 383: ’ok’,
Line 383: ‘tok’,
Line 383: ‘th’,
Line 387: on-the-fly
Line 387: subword
Line 387: tokenised
Line 387: sub-words.
Line 387: subwords
Line 391: subword
Line 391: tokenisation
Line 391: BPE
Line 391: WordPiece.
Line 391: BPE
Line 391: WordPiece.
Line 399: maxiter
Line 403: PREPROCESS(D)
Line 407: maxiter
Line 409: tl
Line 409: (FC
Line 411: tlr
Line 411: tl
Line 413: tl:tr
Line 413: tlr
Line 415: tlr
Line 431: split(D,
Line 445: WordPiece
Line 445: Tokeniser
Line 449: BPE
Line 449: subword’s
Line 449: BPE
Line 449: WordPiece
Line 449: Tokeniser.
Line 449: BPE
Line 449: WordPiece
Line 453: co-occurring
Line 453: WordPiece
Line 457: SentencePiece
Line 457: Tokeniser
Line 459: tokenisation
Line 459: preprocessed
Line 459: Chinese
Line 459: Japanese,
Line 459: language-agnostic/space-agnostic
Line 459: SentencePiece
Line 459: tokeniser
Line 459: SentencePiece
Line 459: tokenisation
Line 459: SentencePiece
Line 459: Unicode
Line 459: Normalization
Line 459: BPE
Line 459: WordPiece,
Line 459: pre-tokenisation
Line 459: SentencePiece
Line 463: Syntactics
Line 465: rules/grammar
Line 465: English
Line 465: (NP)
Line 465: (VP).
Line 477: POS
Line 477: mouseate–cheese-drawer
Line 477: (e.g.,
Line 477: morphologically
Line 477: transition-based
Line 477: graph-based
Line 489: connotation/semantics
Line 489: lexically
Line 493: real-world
Line 493: first-order
Line 493: parsing—decomposition,
Line 497: Decompositional
Line 497: qualities—being
Line 497: first-order
Line 501: ‘money’—dictates
Line 501: existence/usage
Line 501: WordNet
Line 501: English.
Line 501: English
Line 505: (e.g.,
Line 505: co-occurrence,
Line 505: i.e.,
Line 505: co-occurs
Line 505: modern-day
Line 505: NLP.
Line 511: Herbert
Line 511: Clark,
Line 511: (Clark
Line 515: (LM)
Line 515: co-occurrence
Line 515: trained/learned,
Line 515: LM
Line 515: xm,
Line 515: LM
Line 515: 1)thtoken,
Line 515: xm+1
Line 515: i.e.,
Line 515: 1)th
Line 515: vocabulary/lexicon
Line 515: LM
Line 515: 1)th
Line 519: NLP.
Line 519: layman’s
Line 519: 1)th
Line 519: Sam.
Line 519: English
Line 519: (i.e.,
Line 523: Bag-of-Word
Line 523: connotated
Line 523: i.e.,
Line 523: bag-of-word
Line 533: bag-of-words
Line 541: preprocessing
Line 541: (lowercasing,
Line 541: lemmatisation
Line 541: tokenisation,
Line 541: unigram
Line 541: ith
Line 545: ‘yes’—the
Line 545: ‘no’—the
Line 549: S2:v1,
Line 549: i.e.,
Line 549: bag-of-word
Line 559: Alexander
Line 559: Bain
Line 559: William
Line 559: James
Line 559: decision-making.
Line 559: McCulloch,
Line 559: neuroscientist,
Line 559: Walter
Line 559: Pitts,
Line 559: perceptron.
Line 559: Rosenblatt,
Line 559: perceptron.
Line 559: Rosenblatt
Line 559: perceptron
Line 559: Minsky
Line 559: Papert
Line 563: Perceptron
Line 565: perceptron,
Line 571: N-dimensional
Line 571: xN),
Line 571: perceptron
Line 571: w1x1
Line 571: w2x2
Line 571: wnxn,
Line 571: perceptron
Line 571: wN)
Line 573: sgn
Line 573: (wTx
Line 575: sgn(·)
Line 575: signum
Line 579: sgn(·)
Line 585: perceptron
Line 585: perceptron
Line 585: sgn(·)
Line 691: perceptron,
Line 695: sgn′(w1x1
Line 695: w2x2
Line 699: sgn’(·)
Line 711: perceptron
Line 711: sgn'(x1
Line 721: sgn'(x1
Line 767: perceptron
Line 767: sgn'(w1x1
Line 767: w2x2
Line 771: disjunction.
Line 771: perceptron
Line 771: perceptron
Line 771: sgn′(x1
Line 775: perceptron
Line 779: Multilayer
Line 779: Perceptron
Line 781: perceptron
Line 781: neuron-like
Line 781: neuron-like
Line 785: wi
Line 785: ith
Line 785: ith
Line 785: feed-forward
Line 789: feed-forward
Line 789: Multilayer
Line 789: Perceptron
Line 789: (MLP),
Line 789: neuron-like
Line 789: feed-forward
Line 795: Multilayer
Line 795: Perceptron.
Line 801: MLP
Line 801: sgn’(·)
Line 807: MLP
Line 807: perceptrons
Line 807: MLP
Line 807: sgn'(x1
Line 807: sgn'(x1
Line 825: sgn'(x1
Line 827: sgn'(x1
Line 829: sgn'(h1
Line 847: sgn'(0
Line 849: sgn'(0
Line 851: sgn'(1
Line 853: sgn'(1
Line 855: sgn'(0
Line 857: sgn'(0
Line 859: sgn'(1
Line 861: sgn'(1
Line 865: sgn'(0
Line 867: sgn'(1
Line 869: sgn'(1
Line 871: sgn'(1
Line 883: Multilayer
Line 883: Perceptron
Line 889: N-dimensional
Line 889: xN})
Line 889: K-dimensional
Line 889: yK}.
Line 893: parametersas
Line 893: perceptron
Line 893: activations.
Line 893: activations
Line 897: activations
Line 897: h'(·)
Line 897: yk
Line 901: xN
Line 901: yK
Line 907: Sigmoid:
Line 907: sigmoid/logistic
Line 911: Sigmoid
Line 911: sigmoid
Line 911: f'
Line 911: σ'(x)
Line 915: tanh:
Line 915: tanh
Line 915: tanh(·)
Line 919: tanh
Line 919: zero-centred
Line 919: tanh'(x)
Line 919: tanh2(x).
Line 923: Softmax:
Line 923: softmax
Line 923: well-defined
Line 923: Softmax
Line 927: softmax
Line 931: ReLU:
Line 931: ReLU
Line 933: ReLU(x)
Line 935: ReLU
Line 935: sigmoid
Line 935: tanh
Line 935: ReLU
Line 939: GELU:
Line 939: GELU
Line 939: Gaussian
Line 941: GELU(x)
Line 943: GELU
Line 943: GELU
Line 943: ReLU.
Line 947: GLU:
Line 947: GLU
Line 947: sigmodi
Line 949: GLU(x)
Line 949: σ(wx
Line 951: component-wise
Line 951: GLU
Line 951: de-emphasise.
Line 955: sigmoid
Line 955: differentiability.
Line 957: Swish(x)
Line 963: SwiGLU:
Line 963: SwiGLU
Line 963: Swish-Gated
Line 963: GLU
Line 965: SwiGLU(x)
Line 965: Swishβ(wx
Line 969: i.e.,
Line 977: step/iteration
Line 977: E(w).
Line 979: Backpropagation
Line 981: jth
Line 981: ∇Ej
Line 991: kth
Line 991: yk
Line 991: w.r.t
Line 995: wij
Line 995: wij
Line 995: ith
Line 995: jth
Line 995: wij
Line 995: ith
Line 995: jth
Line 995: wij
Line 999: backpropagation.
Line 1003: backpropagation
Line 1003: tanh
Line 1003: N-dimensional
Line 1003: xN)
Line 1003: K-dimensional
Line 1003: yK).
Line 1003: h(x)
Line 1003: tanh(·)
Line 1003: yk
Line 1013: yk
Line 1013: tk
Line 1017: backpropagation
Line 1041: updation
Line 1045: memory-intensive
Line 1045: overfit.
Line 1047: (SGD)
Line 1051: Mini-Batch
Line 1053: SGD,
Line 1053: mini-batching,
Line 1059: Sigmoid
Line 1087: Hyperparameters
Line 1089: (i.e.,
Line 1089: generalisability
Line 1091: underfit
Line 1091: overfit
Line 1091: generalisability.
Line 1093: hyperparameters.
Line 1099: overfitting
Line 1099: underfitting,
Line 1099: MLP
Line 1099: underfitting.
Line 1099: overfit.
Line 1105: iterations/steps
Line 1109: <H4>Learning
Line 1117: Time-Based
Line 1123: overfitting
Line 1127: overfitting
Line 1127: overfit.
Line 1131: overfitting.
Line 1131: Lp
Line 1131: n-dimensional
Line 1131: Manhattan
Line 1133: E(w)
Line 1139: mini-batch
Line 1139: overfitting
Line 1145: derivate
Line 1149: E(w)
Line 1149: ith
Line 1149: w(i),
Line 1149: z(i)
Line 1149: activations
Line 1149: a(i)
Line 1149: z(i)
Line 1149: h(a(i)),
Line 1153: sigmoid
Line 1153: tanh.
Line 1153: ReLU
Line 1167: ith
Line 1167: ith
Line 1183: Positive/Negative.
Line 1183: (TP)
Line 1183: truly/correctly
Line 1187: erroneously/falsely
Line 1187: i.e.,
Line 1187: (FN).
Line 1191: FN
Line 1191: (FP).
Line 1199: correct/incorrect
Line 1199: ith
Line 1199: TP
Line 1199: yi
Line 1199: yi
Line 1199: yi
Line 1279: TP
Line 1281: FN
Line 1283: FP
Line 1285: TP
Line 1289: FP
Line 1291: TP
Line 1293: TP
Line 1295: TP
Line 1297: FN
Line 1301: (TP),
Line 1301: (FP)
Line 1301: (FN)
Line 1315: (TP)
Line 1317: (FN)
Line 1321: (FP)
Line 1331: actual/expected
Line 1337: safe/positive,
Line 1337: FNs.
Line 1337: (FN
Line 1337: tug-of-war,
Line 1337: FN
Line 1337: FP
Line 1337: vice-versa,
Line 1349: NLP
Line 1349: NLP
Line 1349: preprocessing
Line 1349: lemmatisation,
Line 1349: tokenisation.
Line 1349: word/sentence
Line 1351: n-dimensional
Line 1351: NLP,
Line 1351: perceptrons
Line 1351: multi-layer
Line 1351: perceptrons
Line 1351: backpropagation,
Line 1351: hyperparameters
Line 1353: n-grams
Line 1353: bag-of-words
Line 1361: (NLP
Line 1361: https://github.com/NiuTrans/ABigSurvey.
Line 1363: NLP:
Line 1363: https://github.com/keon/awesome-nlp.
Line 1367: Akmajian
Line 1367: et
Line 1367: al.
Line 1369: Mielke,
Line 1369: Sabrina
Line 1369: et
Line 1369: al.
Line 1369: Open-Vocabulary
Line 1369: Modeling
Line 1369: Tokenization
Line 1369: NLP.”
Line 1369: arXiv
Line 1369: preprint
Line 1369: arXiv:2112.10508
Line 1371: Bonan,
Line 1371: et
Line 1371: al.
Line 1371: pre-trained
Line 1371: ACM
Line 1375: Daniel
Line 1375: et
Line 1375: al.
Line 1375: IEEE
Line 1387: Tokenization
Line 1387: Similarityhttps://huggingface.co/spaces/spacy/pipeline-visualizer#en_core_web_lg
Line 1389: https://playground.tensorflow.org/
Line 1391: Optimization
Line 1391: Descenthttps://uclaacm.github.io/gradient-descent-visualiser/#playground
Line 1399: True/False
Line 1401: Lemmatisation
Line 1401: (True/False)
Line 1403: sigmoid
Line 1403: (True/False)
Line 1405: SentencePiece
Line 1405: pre-tokenised.
Line 1405: (True/False)
Line 1407: non-linearity.
Line 1407: (True/False)
Line 1409: (True/False)
Line 1421: Sociolinguistics
Line 1425: Activations
Line 1431: non-linearity
Line 1433: GELU
Line 1439: dimensionality
Line 1441: lemmatisation?
Line 1443: ReLU
Line 1443: sigmoid
Line 1445: NLP
Line 1445: tokenisation,
Line 1445: POS
Line 1445: lemmatisation
Line 1459: tokenisation
Line 1461: preprocessing
Line 1461: NLP
Line 1463: three-input
Line 1463: [w1,w2,w3,
Line 1463: sigmoid
Line 1465: WordNet?
Line 1465: WordNet.
Line 1467: subword
Line 1467: tokenisation?
Line 1467: tokenisation
Line 1469: NLP
Line 1477: sigmoid
Line 1477: σ(wTx
Line 1477: exp(–z)).
Line 1485: Akmajian,
Line 1485: Demers,
Line 1485: Harnish,
Line 1485: MIT
Line 1485: Press.URL
Line 1485: https://doi.org/10.7551/mitpress/4252.001.0001
Line 1487: Berlin,
Line 1487: Heidelberg:
Line 1487: Springer-Verlag.
Line 1489: Cho,
Line 1489: Merriënboer,
Line 1489: Gulcehre,
Line 1489: Bahdanau,
Line 1489: Bougares,
Line 1489: Schwenk,
Line 1489: Bengio,
Line 1489: RNN
Line 1489: encoder–decoder
Line 1489: (EMNLP),
Line 1489: 1724–1734).URL
Line 1489: https://aclanthology.org/D14-1179
Line 1491: Clark,
Line 1493: Eisenstein,
Line 1493: MIT
Line 1495: Elman,
Line 1495: 179–211.URL
Line 1495: https://www.sciencedirect.com/science/article/pii/036402139090002E
Line 1497: Gage,
Line 1499: Gers,
Line 1499: Schmidhuber,
Line 1499: Cummins,
Line 1499: lstm.
Line 1499: Comput.,